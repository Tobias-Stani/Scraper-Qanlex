# üè°Ô∏è **Proyecto de Web Scraping Judicial**

## üìù **Descripci√≥n del Proyecto**
Herramienta automatizada para la extracci√≥n de datos de **expedientes judiciales** del sitio web del **Poder Judicial Nacional**, con almacenamiento en una base de datos **MySQL**.

---

## üóÇÔ∏è **Descripci√≥n Detallada de Scripts**

### 1. `scraper.py`
#### üõ†Ô∏è **Funcionalidad Principal**
- Automatiza la extracci√≥n de informaci√≥n de casos judiciales.

#### üöÄ **Acciones Espec√≠ficas**
- Navega al sitio web del **Poder Judicial Nacional**.
- Realiza b√∫squedas con el filtro de palabra clave **"residuos"**.
- Resuelve el **CAPTCHA** de forma **manual**.
- Extrae informaci√≥n detallada de cada expediente, incluyendo:
  - **N√∫mero de expediente**  
  - **Jurisdicci√≥n**  
  - **Dependencia**  
  - **Situaci√≥n actual**  
  - **Car√°tula** (descripci√≥n del caso)  
  - **Movimientos del expediente**  
  - **Actores y demandados**

#### üìÇ **Salida**
- Genera un archivo `expedientes.json` con todos los datos extra√≠dos.

---

### 2. `guardarDb.py`
#### üõ†Ô∏è **Funcionalidad Principal**
- Automatiza la carga de datos extra√≠dos en una base de datos **MySQL**.

#### üöÄ **Acciones Espec√≠ficas**
- Lee el archivo `expedientes.json`.
- Establece conexi√≥n con la base de datos **MySQL**.
- Inserta los datos en tres tablas relacionales:
  1. **`expedientes`**: Informaci√≥n general del caso.  
  2. **`movimientos`**: Historial de movimientos del expediente.  
  3. **`participantes`**: Listado de actores y demandados. 
- Elimina el archivo.
- Cierra la conexion.  

#### üîß **Procesamiento de Datos**
- Limpia y formatea las fechas.
- Maneja **transacciones SQL** para asegurar la integridad de los datos.
- Elimina el archivo JSON despu√©s de una carga exitosa.

---

## üîß **Requisitos Previos**
Antes de ejecutar el proyecto, aseg√∫rate de tener instalados:

- **Git**
- **Docker** y **Docker Compose**
- **Selenium**: Para la automatizaci√≥n del navegador.
- **Webdriver-Manager**: Gesti√≥n autom√°tica de **ChromeDriver**.
- **MySQL Connector for Python**: Para interactuar con la base de datos.

---

## üöÄ **Pasos de Instalaci√≥n y Ejecuci√≥n**

1. **Clonar el repositorio**:  
   ```bash
   git clone https://github.com/Tobias-Stani/Scraper-Qanlex.git
   ```

2. **Moverse al directorio del proyecto**:  
   ```bash
   cd Scraper-Qanlex
   ```

3. **Construir la imagen de Docker**:  
   ```bash
   docker-compose build
   ```

4. **Levantar el contenedor**:  
   ```bash
   docker-compose up
   ```

5. **Instalar dependencias (local)**:  
   ```bash
   pip install selenium webdriver-manager mysql-connector-python
   ```

6. **Configurar conexi√≥n MySQL**:  
   - Verificar las credenciales y la IP del servidor en el archivo `docker-compose.yml`.
   - Configurar los datos de acceso en los scripts.

7. **Crear base de datos y estructura**:  
   Ejecutar la siguiente estructura SQL en MySQL:

   ```sql
   CREATE TABLE expedientes (
       id INT AUTO_INCREMENT PRIMARY KEY,
       expediente VARCHAR(255),
       jurisdiccion VARCHAR(255),
       dependencia VARCHAR(255),
       situacion_actual VARCHAR(255),
       caratula VARCHAR(255)
   );

   CREATE TABLE movimientos (
       id INT AUTO_INCREMENT PRIMARY KEY,
       expediente_id INT,
       fecha DATE,
       tipo VARCHAR(255),
       detalle TEXT,
       FOREIGN KEY (expediente_id) REFERENCES expedientes(id)
   );

   CREATE TABLE participantes (
       id INT AUTO_INCREMENT PRIMARY KEY,
       expediente_id INT,
       tipo VARCHAR(255),
       nombre VARCHAR(255),
       FOREIGN KEY (expediente_id) REFERENCES expedientes(id)
   );
   ```

8. **Ejecutar los scripts**:  
   ```bash
   python scraper.py
   python guardarDb.py
   ```

---

## üí° **Posibles Mejoras**
1. **Optimizaci√≥n del script principal** para reducir los tiempos de ejecuci√≥n.  
2. Implementaci√≥n de **resoluci√≥n autom√°tica del CAPTCHA**.  
3. Mejor arquitectura para **AWS** con despliegue automatizado.  
4. Uso de **variables de entorno** para eliminar credenciales **hardcodeadas**.

---

## üîç **Detalles del Proyecto**

### üì¶ **Componentes**
- **`scraper.py`**: Extracci√≥n de datos judiciales.  
- **`guardarDb.py`**: Carga de datos en MySQL.  
- **Docker**: Infraestructura para facilitar el despliegue.

### üõ†Ô∏è **Tecnolog√≠as Utilizadas**
- **Python**
- **Selenium**
- **MySQL**
- **Docker**

---

## üö® **Consideraciones Importantes**

- **Resoluci√≥n Manual de CAPTCHA**: 
  - Durante la ejecuci√≥n del script `scraper.py`, se requiere intervenci√≥n manual para resolver el CAPTCHA. Aseg√∫rate de estar disponible para completar esta tarea cuando se te solicite.

- **Una vez resuelto el CAPTCHA**:
  - Una vez resuelto el CAPTCHA, el usuario no necesitar√° hacer nada m√°s. El proceso est√° completamente automatizado y el programa te indicar√° cualquier acci√≥n adicional que deba tomarse.
  
- **Conexi√≥n a Internet Estable**:
  - La herramienta depende de una conexi√≥n a internet para acceder al sitio web y obtener los datos. Aseg√∫rate de tener una conexi√≥n estable antes de ejecutar los scripts.

- **Instalaci√≥n de Google Chrome**:
  - Se debe tener **Google Chrome** instalado en la m√°quina host, ya que **Selenium** utiliza este navegador para la automatizaci√≥n.

- **Ejecutar el Script `scraper.py` Varias Veces**:
  - Es posible que necesites ejecutar el script `scraper.py` varias veces para asegurarte de que se obtengan todos los expedientes disponibles. Esto es especialmente √∫til si el n√∫mero de casos es grande y la b√∫squeda puede no devolver todos los resultados en una sola ejecuci√≥n.

- **Credenciales de Base de Datos**:
  - Aseg√∫rate de configurar correctamente las credenciales de acceso a la base de datos. Por defecto, las credenciales son las siguientes:
  
    ```bash
    DB_HOST=172.30.0.2
    DB_USER=scraperuser
    DB_PASSWORD=scraperpass
    DB_NAME=scraper_data
    ```

  - Verifica que estos valores coincidan con los configurados en tu entorno o en tu archivo `docker-compose.yml`.



